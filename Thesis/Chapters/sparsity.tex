\section{Introduction to Sparisty Analysis}

Given the prevalence and the strength of noise in weak lensing analysis, to successfully reconstruct physical information from weak lensing data, one may be prompted to use \emph{sparsity}. A \emph{sparse} model indicates only a relatively small amount of parameters controls
the behavior of the system. In weak lensing and specifically 3D reconstruction, we apply this idea to assert that galaxy clusters are only sparsely 
distributed in the present universe. 

To implement the sparisty prior in an analysis, first consider the familiar linear regression model: 

\begin{equation}
  y_i = \beta_0 + \sum^{p}_{j=1} x_{ij}\beta_j + e_i
\end{equation}
where $\beta_0$ and $\beta = (\beta_1, \beta_2, \ldots, \beta_p)$ are unknown multidimensional linear parameters of some model. $y_i$'s are measured values, and $e_i$'s are errors of estimation.

To retrieve the best predicted model, one acquire an estimation of $\beta$ parameters by mininizing the least-squares objective function. 
\begin{equation}
  \mathrm{minimize}_{\beta, \beta_0} \sum^N_{i=1}(y_i - \beta_0 - \sum^{p}_{j=1}x_{ij}\beta_j)^2
\end{equation}

However, when the number of undertermined parameters is large, such an objective function may yield nonzero value for the $\beta$ vector and yield degenerate solutions. This makes the interpretation of the result difficult. 
A solution is just to \emph{regularize} the estimation process. The regime we will be working with in this work is called the Lasso regularization, which uses the Lasso Estimator:

\begin{equation}
  \mathrm{minimize}_{\beta_0, \beta}\{\frac{1}{2N} \sum^N_{i=1}(y_i-\beta_0 - \sum^{p}_{j=1}x_{ij}\beta_j)^2\}
\end{equation}
subject to 
\[\lVert \beta \lVert_1\leq t.\]
Where $\lVert \beta \lVert_1$ represents the $l_1$-norm which is the sum of the absolute values of each entries in $\beta$. 

In matrix form, one may have
\begin{equation}
  \mathrm{minimize}_{\beta_0, \beta}\Bigl\{\frac{1}{2N} \lVert \bf{y} - \beta_0 \bf{1} - \bf{X}\beta\lVert^2_2\Bigl\}
\end{equation}

subject to 
\[\lVert \beta \lVert_1\leq t.\] 

One can also rewrite this into the Lagrangian form
\begin{equation}
  \mathrm{minimize}_{\beta\in \mathbb{R}^p}\Bigl\{ \frac{1}{2N}\lVert \bf{y} - \bf{X}\beta \lVert^2_2 + \lambda \lVert\beta \lVert_1\Bigl\}
  \label{Lasso Regression}
\end{equation}

Here then $\lambda \geq 0$ now controls the degree of sparisty we impose on the minimization, with larger $\lambda$ correspond to stronger sparse condition. For our application, we will use a slightly modified version of the Lasso regression regime where $\lambda$ is 
normalized with respect to standard deviation of the underlying shear field estimation. 

\section{Adaptative Lasso Regression}
In this section we describe the Adaptive Lasso Regression algorithm introduced in \cite{AdaLASSO-Zou2006}. In (Fan and LI 2001), it was discovered the lasso algorithm has biased estimates for large coefficients. Meinshausen and BÃ¼hlmann (2004) also sproved even the optimal $\lambda$ value yields in consistent variable selection results, meaning that the regression algorithm sometimes picks up signal from the noise. These are all signs that the  ``traditional'' lasso regression in Eqn. \ref{Lasso Regression} does not satisfy the oracle property, which is the tendancy that a penalized (e.g. the Lasso $l_1$ norm penalty or the Smoothly Clipped Absolute Deviation penalty) regression estimator being able to recover the true underlying model consistently. The nature of the adaptive Lasso algorithm is to use an adaptive weight to penalize different projection coeffcients in the $l_1$ penalty. 

The adaptive lasso regression algorithm can be described as follows. After getting the Lasso estimator $\hat{\beta}_{\mathrm{Lasso}}$ by minimizing Eqn. \ref{Lasso Regression}, we then define the adaptive weight, with $\tau=2$ in this work (although it can be any postive real number):

\begin{equation}
  \hat{w} = \frac{1}{\lvert \hat{\beta}_{\mathrm{Lasso}} \rvert^\tau}.
  \label{weightFactor}
\end{equation}
This adaptive weight enhance the shrinkage in for the coefficients with smaller amplitudes, as can be seen the inverse of the norm of the preliminary estimates. 
The adaptive Lasso estimator is then acquire by minimizing 
\begin{equation}
  \hat{\beta'} =  \mathrm{arg}\text{ } \mathrm{minimize}_{\beta'\in \mathbb{R}^p}\Bigl\{ \frac{1}{2}\sum\lVert \bf{y} - \bf{X}\beta' \lVert^2_2 + \lambda_{\mathrm{ada}} \lVert\hat{w} \circ \beta' \lVert_1\Bigl\},
  \label{adaptiveLasso}
\end{equation}
where $\lambda_{\mathrm{ada}}$ is the adaptive Lasso penalty parameter, which we have taken to be the same as $\lambda$ in Eqn. \ref{Lasso Regression}. ``$\circ$'' refers to the element-wise product. 